"""
Remix Engine: Vibe-Based Video Creation with Surgical API Orchestration

This module handles the complete Remix pipeline:
1. Vibe Extraction - Analyze reference files for mood/energy/pacing (NOT for integration)
2. Topic Research - How internet audiences prefer content about the topic
3. Runway API - Generate/transform visuals based on vibe
4. Stock Integration - Fill gaps, blended creatively by Runway
5. Shotstack API - Precise JSON instructions for final assembly

File Type Distinction:
- Reference Files: Vibe extraction ONLY, never integrated into final video
- Content Files: User's actual content to prioritize in final video
- Stock Assets: Fill gaps, transformed through Runway for cohesion
- Runway Output: AI-generated scenes forming the visual core
"""

import os
import json
import requests
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, asdict
from enum import Enum

class FileType(Enum):
    REFERENCE = "reference"  # For vibe extraction only
    CONTENT = "content"      # To be integrated into final video
    STOCK = "stock"          # From stock libraries
    RUNWAY = "runway"        # AI-generated by Runway
    GENERATED = "generated"  # DALL-E or other AI images


class QualityTier(Enum):
    """Video generation quality tiers with Runway model mapping."""
    GOOD = "good"
    BETTER = "better"
    BEST = "best"


QUALITY_TIERS = {
    QualityTier.GOOD: {
        "model": "gen3a_turbo",
        "name": "Good",
        "price_per_30s": 4.00,
        "description": "Fast generation, solid quality for most content",
        "cost_per_second": 0.05,
    },
    QualityTier.BETTER: {
        "model": "gen4_turbo",
        "name": "Better",
        "price_per_30s": 6.00,
        "description": "Enhanced detail and motion consistency",
        "cost_per_second": 0.10,
    },
    QualityTier.BEST: {
        "model": "gen4_aleph",
        "name": "Best",
        "price_per_30s": 8.00,
        "description": "Cinema-grade output with maximum fidelity",
        "cost_per_second": 0.15,
    },
}

QUALITY_DISCLAIMER = "Quality tier affects visual generation only. It won't change your video's direction, pacing, or message — just how sharp and polished the final output looks."


def get_quality_tier_info(tier: QualityTier) -> Dict[str, Any]:
    """Get full info for a quality tier."""
    return QUALITY_TIERS.get(tier, QUALITY_TIERS[QualityTier.GOOD])


def get_runway_model_for_tier(tier: QualityTier) -> str:
    """Get the Runway model ID for a quality tier."""
    return QUALITY_TIERS.get(tier, QUALITY_TIERS[QualityTier.GOOD])["model"]


def calculate_cost_for_tier(tier: QualityTier, duration_seconds: float) -> float:
    """Calculate total cost for a video at given quality tier."""
    tier_info = QUALITY_TIERS.get(tier, QUALITY_TIERS[QualityTier.GOOD])
    runway_cost = tier_info["cost_per_second"] * duration_seconds
    shotstack_cost = 0.20 * (duration_seconds / 30)
    claude_cost = 0.50
    elevenlabs_cost = 0.30
    stock_cost = 0.10
    return round(runway_cost + shotstack_cost + claude_cost + elevenlabs_cost + stock_cost, 2)


@dataclass
class VibeProfile:
    """Extracted vibe characteristics from reference material."""
    mood: str           # calm, energetic, provocative, mysterious, etc.
    energy_level: float # 0.0 (slow/meditative) to 1.0 (high-energy)
    pacing: str         # slow, moderate, fast, variable
    color_feel: str     # warm, cool, neutral, vibrant, muted
    cut_rhythm: str     # steady, punchy, flowing, chaotic
    audio_vibe: str     # ambient, upbeat, dramatic, minimal
    visual_density: str # sparse, balanced, dense
    emotional_arc: str  # build, release, constant, wave


@dataclass 
class ShotStackClip:
    """Precise clip instruction for Shotstack API."""
    asset_url: str
    asset_type: str     # video, image, audio, text
    start_time: float   # seconds
    duration: float     # seconds
    track: int          # layer number (0 = bottom)
    position_x: float   # 0.0 to 1.0
    position_y: float   # 0.0 to 1.0
    scale: float        # 1.0 = 100%
    opacity: float      # 0.0 to 1.0
    transition_in: str  # fade, slide, zoom, none
    transition_out: str
    effects: List[str]  # color_grade, blur, etc.


@dataclass
class RunwayInstruction:
    """Precise instruction for Runway API generation."""
    scene_id: int
    generation_type: str  # image_to_video, text_to_video
    prompt: str
    duration: float       # seconds
    style_reference: str  # style description from vibe
    motion_guidance: str  # how motion should flow
    blend_with: List[str] # stock asset IDs to blend


@dataclass
class OrchestrationPlan:
    """Complete surgical plan for all APIs working in conjunction."""
    vibe_profile: VibeProfile
    runway_instructions: List[RunwayInstruction]
    stock_queries: List[Dict[str, Any]]
    shotstack_timeline: List[ShotStackClip]
    total_duration: float
    estimated_cost: float


# API Configuration
RUNWAY_API_KEY = os.environ.get("RUNWAY_API_KEY")
SHOTSTACK_API_KEY = os.environ.get("SHOTSTACK_API_KEY")
SHOTSTACK_ENV = os.environ.get("SHOTSTACK_ENV", "stage")  # stage or v1

RUNWAY_BASE_URL = "https://api.dev.runwayml.com/v1"
RUNWAY_API_VERSION = "2024-11-06"
SHOTSTACK_BASE_URL = f"https://api.shotstack.io/{SHOTSTACK_ENV}"


class RunwayTaskStatus(Enum):
    """Runway API task status codes."""
    PENDING = "PENDING"
    RUNNING = "RUNNING"
    SUCCEEDED = "SUCCEEDED"
    FAILED = "FAILED"
    CANCELLED = "CANCELLED"


class RunwayError(Exception):
    """Custom exception for Runway API errors."""
    def __init__(self, message: str, code: Optional[str] = None, status_code: Optional[int] = None):
        self.message = message
        self.code = code
        self.status_code = status_code
        super().__init__(self.message)


@dataclass
class RunwayTaskResult:
    """Result from a Runway API task."""
    task_id: str
    status: RunwayTaskStatus
    progress: Optional[float]
    output_urls: Optional[List[str]]
    error_message: Optional[str]
    error_code: Optional[str]


def runway_create_image_to_video(
    prompt_image: str,
    prompt_text: str,
    quality_tier: QualityTier = QualityTier.GOOD,
    duration: int = 5,
    ratio: str = "9:16"
) -> RunwayTaskResult:
    """
    Create an image-to-video generation task via Runway API.
    
    Endpoint: POST /v1/image_to_video
    
    Args:
        prompt_image: Image URL (HTTPS, Runway URI, or data URI)
        prompt_text: Motion/style description (max 1000 chars)
        quality_tier: QualityTier enum for model selection
        duration: 5 or 10 seconds
        ratio: Aspect ratio (16:9, 9:16, 1:1)
        
    Returns:
        RunwayTaskResult with task_id and initial status
        
    Raises:
        RunwayError: On API failure
    """
    if not RUNWAY_API_KEY:
        raise RunwayError("RUNWAY_API_KEY not configured", code="MISSING_API_KEY")
    
    model = get_runway_model_for_tier(quality_tier)
    
    headers = {
        "Authorization": f"Bearer {RUNWAY_API_KEY}",
        "X-Runway-Version": RUNWAY_API_VERSION,
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": model,
        "promptImage": prompt_image,
        "promptText": prompt_text[:1000],
        "duration": duration,
        "ratio": ratio,
        "watermark": False
    }
    
    print(f"[Runway API] Creating image_to_video task with model={model}")
    
    try:
        response = requests.post(
            f"{RUNWAY_BASE_URL}/image_to_video",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        if response.status_code == 401:
            raise RunwayError("Invalid API key", code="UNAUTHORIZED", status_code=401)
        elif response.status_code == 429:
            raise RunwayError("Rate limit exceeded", code="RATE_LIMITED", status_code=429)
        elif response.status_code == 400:
            error_data = response.json() if response.text else {}
            error_code = error_data.get("code", "BAD_REQUEST")
            if error_code == "CONTENT_MODERATED" or "moderat" in error_data.get("message", "").lower():
                raise RunwayError(
                    "Content was flagged by moderation - please use different source imagery",
                    code="CONTENT_MODERATED",
                    status_code=400
                )
            raise RunwayError(
                error_data.get("message", "Bad request"),
                code=error_code,
                status_code=400
            )
        elif response.status_code not in [200, 201, 202]:
            raise RunwayError(
                f"Unexpected response: {response.status_code}",
                code="API_ERROR",
                status_code=response.status_code
            )
        
        data = response.json()
        
        return RunwayTaskResult(
            task_id=data.get("id", ""),
            status=RunwayTaskStatus(data.get("status", "PENDING")),
            progress=data.get("progress"),
            output_urls=data.get("output"),
            error_message=data.get("failure"),
            error_code=data.get("failureCode")
        )
        
    except requests.exceptions.Timeout:
        raise RunwayError("Request timed out", code="TIMEOUT")
    except requests.exceptions.RequestException as e:
        raise RunwayError(f"Network error: {str(e)}", code="NETWORK_ERROR")


def runway_get_task_status(task_id: str) -> RunwayTaskResult:
    """
    Get status of a Runway generation task.
    
    Endpoint: GET /v1/tasks/{task_id}
    
    Args:
        task_id: The task ID from create call
        
    Returns:
        RunwayTaskResult with current status and output if complete
        
    Raises:
        RunwayError: On API failure
    """
    if not RUNWAY_API_KEY:
        raise RunwayError("RUNWAY_API_KEY not configured", code="MISSING_API_KEY")
    
    headers = {
        "Authorization": f"Bearer {RUNWAY_API_KEY}",
        "X-Runway-Version": RUNWAY_API_VERSION
    }
    
    try:
        response = requests.get(
            f"{RUNWAY_BASE_URL}/tasks/{task_id}",
            headers=headers,
            timeout=30
        )
        
        if response.status_code == 404:
            raise RunwayError("Task not found", code="NOT_FOUND", status_code=404)
        elif response.status_code != 200:
            raise RunwayError(
                f"Failed to get task status: {response.status_code}",
                code="API_ERROR",
                status_code=response.status_code
            )
        
        data = response.json()
        
        return RunwayTaskResult(
            task_id=task_id,
            status=RunwayTaskStatus(data.get("status", "PENDING")),
            progress=data.get("progress"),
            output_urls=data.get("output"),
            error_message=data.get("failure"),
            error_code=data.get("failureCode")
        )
        
    except requests.exceptions.Timeout:
        raise RunwayError("Request timed out", code="TIMEOUT")
    except requests.exceptions.RequestException as e:
        raise RunwayError(f"Network error: {str(e)}", code="NETWORK_ERROR")


def runway_wait_for_completion(
    task_id: str,
    max_wait_seconds: int = 300,
    poll_interval: int = 5
) -> RunwayTaskResult:
    """
    Poll Runway task until completion or timeout.
    
    Args:
        task_id: The task ID to poll
        max_wait_seconds: Maximum time to wait (default 5 minutes)
        poll_interval: Seconds between polls (default 5)
        
    Returns:
        Final RunwayTaskResult
        
    Raises:
        RunwayError: On failure or timeout
    """
    import time
    
    elapsed = 0
    while elapsed < max_wait_seconds:
        result = runway_get_task_status(task_id)
        
        print(f"[Runway API] Task {task_id[:8]}... status={result.status.value}, progress={result.progress}")
        
        if result.status == RunwayTaskStatus.SUCCEEDED:
            print(f"[Runway API] Task completed! Output URLs: {result.output_urls}")
            return result
        elif result.status == RunwayTaskStatus.FAILED:
            raise RunwayError(
                result.error_message or "Task failed",
                code=result.error_code or "TASK_FAILED"
            )
        elif result.status == RunwayTaskStatus.CANCELLED:
            raise RunwayError("Task was cancelled", code="CANCELLED")
        
        time.sleep(poll_interval)
        elapsed += poll_interval
    
    raise RunwayError(f"Task timed out after {max_wait_seconds}s", code="TIMEOUT")


def runway_generate_video(
    prompt_image: str,
    prompt_text: str,
    quality_tier: QualityTier = QualityTier.GOOD,
    duration: int = 5,
    ratio: str = "9:16",
    wait_for_completion: bool = True
) -> Dict[str, Any]:
    """
    High-level function to generate a video via Runway.
    Handles create + poll in one call.
    
    Args:
        prompt_image: Source image URL
        prompt_text: Motion description
        quality_tier: Quality tier enum
        duration: 5 or 10 seconds
        ratio: Aspect ratio
        wait_for_completion: Whether to poll until done
        
    Returns:
        Dict with task_id, status, and output_url if complete
    """
    try:
        result = runway_create_image_to_video(
            prompt_image=prompt_image,
            prompt_text=prompt_text,
            quality_tier=quality_tier,
            duration=duration,
            ratio=ratio
        )
        
        if not wait_for_completion:
            return {
                "success": True,
                "task_id": result.task_id,
                "status": result.status.value,
                "output_url": None
            }
        
        final_result = runway_wait_for_completion(result.task_id)
        
        return {
            "success": True,
            "task_id": final_result.task_id,
            "status": final_result.status.value,
            "output_url": final_result.output_urls[0] if final_result.output_urls else None,
            "all_outputs": final_result.output_urls
        }
        
    except RunwayError as e:
        return {
            "success": False,
            "error": e.message,
            "error_code": e.code,
            "status_code": e.status_code
        }


def runway_generate_with_retry(
    prompt_image: str,
    prompt_text: str,
    quality_tier: QualityTier = QualityTier.GOOD,
    duration: int = 5,
    ratio: str = "9:16",
    max_retries: int = 3,
    base_delay: float = 5.0
) -> Dict[str, Any]:
    """
    Generate video with automatic retry and exponential backoff for rate limits.
    
    Retry delays: 5s -> 10s -> 20s (exponential backoff)
    
    Args:
        prompt_image: Source image URL
        prompt_text: Motion description
        quality_tier: Quality tier enum
        duration: 5 or 10 seconds
        ratio: Aspect ratio
        max_retries: Maximum retry attempts (default 3)
        base_delay: Initial retry delay in seconds (default 5)
        
    Returns:
        Dict with success status and output URL or user-friendly error
    """
    import time
    
    last_error = None
    
    for attempt in range(max_retries + 1):
        try:
            result = runway_create_image_to_video(
                prompt_image=prompt_image,
                prompt_text=prompt_text,
                quality_tier=quality_tier,
                duration=duration,
                ratio=ratio
            )
            
            final_result = runway_wait_for_completion(result.task_id)
            
            return {
                "success": True,
                "task_id": final_result.task_id,
                "status": final_result.status.value,
                "output_url": final_result.output_urls[0] if final_result.output_urls else None,
                "all_outputs": final_result.output_urls,
                "retries_used": attempt
            }
            
        except RunwayError as e:
            last_error = e
            
            if e.code == "RATE_LIMITED" and attempt < max_retries:
                delay = base_delay * (2 ** attempt)
                print(f"[Runway Queue] Rate limited, retrying in {delay}s (attempt {attempt + 1}/{max_retries})")
                time.sleep(delay)
                continue
            elif e.code == "CONTENT_MODERATED":
                return {
                    "success": False,
                    "error": "The image couldn't be processed. Please try a different source image.",
                    "error_code": e.code,
                    "user_message": "Content couldn't be processed"
                }
            elif e.code == "TIMEOUT":
                if attempt < max_retries:
                    delay = base_delay * (2 ** attempt)
                    print(f"[Runway Queue] Timeout, retrying in {delay}s (attempt {attempt + 1}/{max_retries})")
                    time.sleep(delay)
                    continue
                return {
                    "success": False,
                    "error": "Generation is taking longer than expected. Your video is queued and will be ready shortly.",
                    "error_code": e.code,
                    "user_message": "High demand — video queued"
                }
            else:
                break
    
    return {
        "success": False,
        "error": last_error.message if last_error else "Unknown error",
        "error_code": last_error.code if last_error else "UNKNOWN",
        "user_message": "Something went wrong. Please try again in a moment."
    }


class RunwayQueue:
    """
    Sequential request queue to prevent rate limiting.
    Processes one Runway request at a time with delays between requests.
    """
    
    def __init__(self, delay_between_requests: float = 2.0):
        self.delay = delay_between_requests
        self._last_request_time = 0.0
    
    def process(
        self,
        prompt_image: str,
        prompt_text: str,
        quality_tier: QualityTier = QualityTier.GOOD,
        duration: int = 5,
        ratio: str = "9:16"
    ) -> Dict[str, Any]:
        """
        Process a single request, respecting rate limits.
        """
        import time
        
        now = time.time()
        time_since_last = now - self._last_request_time
        
        if time_since_last < self.delay:
            sleep_time = self.delay - time_since_last
            print(f"[Runway Queue] Waiting {sleep_time:.1f}s before next request...")
            time.sleep(sleep_time)
        
        self._last_request_time = time.time()
        
        return runway_generate_with_retry(
            prompt_image=prompt_image,
            prompt_text=prompt_text,
            quality_tier=quality_tier,
            duration=duration,
            ratio=ratio
        )
    
    def process_batch(
        self,
        requests: List[Dict[str, Any]],
        quality_tier: QualityTier = QualityTier.GOOD,
        on_progress: Any = None
    ) -> List[Dict[str, Any]]:
        """
        Process multiple requests sequentially with progress callbacks.
        
        Args:
            requests: List of dicts with prompt_image, prompt_text, duration, ratio
            quality_tier: Quality tier for all requests
            on_progress: Optional callback(current, total, status_message)
            
        Returns:
            List of results for each request
        """
        results = []
        total = len(requests)
        
        for i, req in enumerate(requests):
            if on_progress:
                remaining_time = (total - i) * 1.5
                on_progress(i + 1, total, f"Generating scene {i + 1} of {total}... ~{int(remaining_time)} min remaining")
            
            result = self.process(
                prompt_image=req.get("prompt_image", ""),
                prompt_text=req.get("prompt_text", ""),
                quality_tier=quality_tier,
                duration=req.get("duration", 5),
                ratio=req.get("ratio", "9:16")
            )
            results.append(result)
            
            if not result.get("success") and result.get("error_code") == "CONTENT_MODERATED":
                print(f"[Runway Queue] Scene {i + 1} failed moderation, continuing with next...")
        
        if on_progress:
            on_progress(total, total, "All scenes generated!")
        
        return results


RUNWAY_QUEUE = RunwayQueue(delay_between_requests=2.0)


def extract_vibe_from_reference(
    reference_file: Optional[Dict[str, Any]] = None,
    user_description: Optional[str] = None
) -> VibeProfile:
    """
    Extract vibe characteristics from a reference file.
    This is for DIRECTION ONLY - the reference is never integrated into the final video.
    
    Args:
        reference_file: Dict with 'url', 'type', 'name' of reference material
        user_description: Optional text description of desired vibe
        
    Returns:
        VibeProfile with extracted characteristics
    """
    from context_engine import call_ai, SYSTEM_GUARDRAILS
    
    file_context = ""
    if reference_file:
        file_context = f"""
REFERENCE FILE (for vibe extraction only, NOT for integration):
- Name: {reference_file.get('name', 'unknown')}
- Type: {reference_file.get('type', 'video')}
- URL: {reference_file.get('url', '')}
"""
    
    user_context = ""
    if user_description:
        user_context = f"\nUSER'S VIBE DESCRIPTION: {user_description}"
    
    prompt = f"""Analyze this reference material to extract VIBE characteristics only.

IMPORTANT: This reference is for DIRECTION/MOOD extraction. It will NOT be integrated into the final video.
The goal is to understand the energy, pacing, and feel so we can CREATE NEW content that matches this vibe.
{file_context}{user_context}

Extract the following vibe characteristics:

1. MOOD: What emotional tone does this convey? (calm, energetic, provocative, mysterious, inspirational, urgent, playful, serious, intimate, epic)

2. ENERGY_LEVEL: Rate 0.0 to 1.0
   - 0.0-0.3: Slow, meditative, contemplative
   - 0.4-0.6: Moderate, balanced, conversational
   - 0.7-1.0: High-energy, fast-paced, intense

3. PACING: How do cuts and transitions flow?
   - slow: Long takes, gentle transitions
   - moderate: Balanced rhythm
   - fast: Quick cuts, dynamic
   - variable: Builds and releases

4. COLOR_FEEL: Overall color palette feeling
   - warm: Golden, orange, cozy
   - cool: Blue, teal, clinical
   - neutral: Balanced, natural
   - vibrant: Saturated, punchy
   - muted: Desaturated, film-like

5. CUT_RHYTHM: How edits land
   - steady: Predictable, consistent
   - punchy: Sharp, impactful
   - flowing: Smooth, continuous
   - chaotic: Unpredictable, energetic

6. AUDIO_VIBE: What audio would complement
   - ambient: Atmospheric, background
   - upbeat: Energetic, driving
   - dramatic: Cinematic, tension
   - minimal: Sparse, focused

7. VISUAL_DENSITY: How much is happening on screen
   - sparse: Clean, minimal elements
   - balanced: Moderate complexity
   - dense: Layered, busy

8. EMOTIONAL_ARC: How emotion flows through
   - build: Starts quiet, grows
   - release: Starts intense, calms
   - constant: Maintains level
   - wave: Rises and falls

Output JSON:
{{
    "mood": "string",
    "energy_level": 0.0-1.0,
    "pacing": "slow|moderate|fast|variable",
    "color_feel": "warm|cool|neutral|vibrant|muted",
    "cut_rhythm": "steady|punchy|flowing|chaotic",
    "audio_vibe": "ambient|upbeat|dramatic|minimal",
    "visual_density": "sparse|balanced|dense",
    "emotional_arc": "build|release|constant|wave"
}}"""

    result = call_ai(prompt, SYSTEM_GUARDRAILS, json_output=True, max_tokens=512)
    
    if result:
        return VibeProfile(
            mood=result.get("mood", "balanced"),
            energy_level=float(result.get("energy_level", 0.5)),
            pacing=result.get("pacing", "moderate"),
            color_feel=result.get("color_feel", "neutral"),
            cut_rhythm=result.get("cut_rhythm", "steady"),
            audio_vibe=result.get("audio_vibe", "ambient"),
            visual_density=result.get("visual_density", "balanced"),
            emotional_arc=result.get("emotional_arc", "constant")
        )
    
    return VibeProfile(
        mood="balanced",
        energy_level=0.5,
        pacing="moderate",
        color_feel="neutral",
        cut_rhythm="steady",
        audio_vibe="ambient",
        visual_density="balanced",
        emotional_arc="constant"
    )


def research_topic_presentation(topic: str, vibe: VibeProfile) -> Dict[str, Any]:
    """
    Research how the internet prefers to consume content about this topic.
    Combines vibe direction with audience preference research.
    
    Args:
        topic: The user's content topic
        vibe: Extracted vibe profile to maintain
        
    Returns:
        Presentation preferences that inform visual decisions
    """
    from context_engine import call_ai, SYSTEM_GUARDRAILS
    from duckduckgo_search import DDGS
    
    search_results = []
    try:
        with DDGS() as ddgs:
            queries = [
                f"{topic} viral video format 2025",
                f"{topic} TikTok Reels what works",
                f"how to present {topic} short form video"
            ]
            for query in queries:
                results = list(ddgs.text(query, max_results=3))
                for r in results:
                    search_results.append({
                        "title": r.get("title", ""),
                        "snippet": r.get("body", ""),
                        "source": r.get("href", "")
                    })
    except Exception as e:
        print(f"[TopicResearch] Search error: {e}")
    
    search_context = "\n".join([
        f"- {r['title']}: {r['snippet'][:150]}"
        for r in search_results[:8]
    ])
    
    prompt = f"""Research how audiences prefer to consume content about "{topic}".

VIBE DIRECTION (must maintain):
- Mood: {vibe.mood}
- Energy: {vibe.energy_level}
- Pacing: {vibe.pacing}

WEB RESEARCH:
{search_context if search_context else "No specific research available - use general short-form best practices"}

Based on this, determine:

1. VISUAL APPROACH: What types of visuals work for this topic?
2. HOOK STYLE: What opening grabs attention for this topic?
3. INFORMATION DENSITY: How much detail per second?
4. B-ROLL PATTERNS: What supporting visuals are expected?
5. TEXT USAGE: How much on-screen text is normal?
6. PACING PREFERENCES: Topic-specific pacing notes

IMPORTANT: Recommendations must WORK WITH the vibe direction, not override it.
The vibe sets the feel. The research sets the format.

Output JSON:
{{
    "visual_approach": "what visual style works for this topic",
    "hook_style": "how to open videos about this topic",
    "information_density": "low|medium|high",
    "broll_patterns": ["type of b-roll 1", "type of b-roll 2"],
    "text_usage": "minimal|moderate|heavy",
    "pacing_notes": "specific pacing recommendations",
    "avoid": ["what doesn't work for this topic"],
    "successful_patterns": ["pattern that works", "another pattern"]
}}"""

    result = call_ai(prompt, SYSTEM_GUARDRAILS, json_output=True, max_tokens=768)
    
    return result if result else {
        "visual_approach": "Clean, professional visuals",
        "hook_style": "Direct statement or question",
        "information_density": "medium",
        "broll_patterns": ["relevant footage", "supporting imagery"],
        "text_usage": "moderate",
        "pacing_notes": "Match content complexity",
        "avoid": [],
        "successful_patterns": []
    }


def generate_runway_instructions(
    vibe: VibeProfile,
    presentation: Dict[str, Any],
    script_segments: Optional[List[Dict[str, Any]]] = None,
    stock_assets: Optional[List[Dict[str, Any]]] = None
) -> List[RunwayInstruction]:
    """
    Generate precise Runway API instructions based on vibe and presentation research.
    Runway will generate/transform visuals and blend stock footage.
    
    Args:
        vibe: Extracted vibe profile
        presentation: Topic presentation research
        script_segments: List of script segments with timing
        stock_assets: Stock assets to potentially blend
        
    Returns:
        List of precise Runway instructions
    """
    from context_engine import call_ai, SYSTEM_GUARDRAILS
    
    stock_context = ""
    if stock_assets:
        stock_context = f"""
STOCK ASSETS AVAILABLE FOR BLENDING:
{json.dumps([{'id': s.get('id'), 'description': s.get('description', '')[:50]} for s in stock_assets[:5]], indent=2)}
"""
    
    prompt = f"""Generate precise Runway API instructions for video generation.

VIBE PROFILE:
- Mood: {vibe.mood}
- Energy: {vibe.energy_level}
- Pacing: {vibe.pacing}
- Color Feel: {vibe.color_feel}
- Cut Rhythm: {vibe.cut_rhythm}

PRESENTATION RESEARCH:
{json.dumps(presentation, indent=2)}

SCRIPT SEGMENTS:
{json.dumps((script_segments or [])[:10], indent=2)}
{stock_context}

Generate Runway instructions for each scene. Each instruction must be:
1. PRECISE - Exact prompt that Runway can execute
2. VIBE-ALIGNED - Matches the mood and energy
3. COHESIVE - Works with other scenes as a whole
4. STOCK-AWARE - Note which stock assets to blend if applicable

For each scene, create:
- generation_type: "image_to_video" (from stock/DALL-E base) or "text_to_video" (pure generation)
- prompt: Detailed visual description for Runway
- duration: Exact seconds
- style_reference: How this matches the vibe
- motion_guidance: How motion should flow (camera movement, subject movement)
- blend_with: IDs of stock assets to incorporate (empty if pure generation)

Output JSON:
{{
    "runway_instructions": [
        {{
            "scene_id": 1,
            "generation_type": "image_to_video",
            "prompt": "Detailed Runway prompt here",
            "duration": 5.0,
            "style_reference": "How this matches {vibe.mood} mood",
            "motion_guidance": "Slow push in, subject remains centered",
            "blend_with": []
        }}
    ],
    "total_runway_seconds": 30,
    "style_notes": "Overall style coherence notes"
}}"""

    result = call_ai(prompt, SYSTEM_GUARDRAILS, json_output=True, max_tokens=1500)
    
    instructions = []
    if result and result.get("runway_instructions"):
        for instr in result["runway_instructions"]:
            instructions.append(RunwayInstruction(
                scene_id=instr.get("scene_id", 0),
                generation_type=instr.get("generation_type", "text_to_video"),
                prompt=instr.get("prompt", ""),
                duration=float(instr.get("duration", 5.0)),
                style_reference=instr.get("style_reference", ""),
                motion_guidance=instr.get("motion_guidance", ""),
                blend_with=instr.get("blend_with", [])
            ))
    
    return instructions


def filter_reference_files(files: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    """
    GUARDRAIL: Filter out reference files from any asset list.
    Reference files are for vibe extraction ONLY and must never enter the timeline.
    
    Args:
        files: List of files that may include reference files
        
    Returns:
        List with reference files removed
    """
    if not files:
        return []
    
    filtered = []
    for f in files:
        file_type = f.get("file_type", f.get("type", "content"))
        if file_type == FileType.REFERENCE.value or file_type == "reference":
            print(f"[GUARDRAIL] Excluding reference file from timeline: {f.get('name', 'unknown')}")
            continue
        filtered.append(f)
    
    return filtered


def prioritize_content_files(
    content_files: List[Dict[str, Any]],
    runway_outputs: List[Dict[str, Any]],
    stock_assets: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    GUARDRAIL: Ensure content files have priority in the asset order.
    Priority: Content files > Runway outputs > Stock assets
    
    Args:
        content_files: User's actual content to prioritize
        runway_outputs: Generated Runway videos
        stock_assets: Stock footage/images
        
    Returns:
        Ordered list with content files first
    """
    prioritized = []
    
    for f in content_files:
        f["priority"] = 1
        f["source"] = "content"
        prioritized.append(f)
    
    for f in runway_outputs:
        f["priority"] = 2
        f["source"] = "runway"
        prioritized.append(f)
    
    for f in stock_assets:
        f["priority"] = 3
        f["source"] = "stock"
        prioritized.append(f)
    
    return prioritized


def generate_shotstack_timeline(
    vibe: VibeProfile,
    runway_outputs: Optional[List[Dict[str, Any]]] = None,
    stock_assets: Optional[List[Dict[str, Any]]] = None,
    content_files: Optional[List[Dict[str, Any]]] = None,
    audio_track: Optional[Dict[str, Any]] = None,
    captions: Optional[List[Dict[str, Any]]] = None
) -> List[ShotStackClip]:
    """
    Generate precise Shotstack timeline with exact frame/layer/timing/position.
    This is the surgical assembly instruction set.
    
    GUARDRAILS APPLIED:
    1. Reference files are automatically filtered out
    2. Content files are prioritized over Runway and Stock
    3. Each asset is tagged with its source for traceability
    
    Args:
        vibe: Vibe profile for transition/effect decisions
        runway_outputs: Generated Runway video URLs
        stock_assets: Stock footage/images
        content_files: User's actual content to prioritize (NOT reference files)
        audio_track: Main audio
        captions: Caption data with timing
        
    Returns:
        Precise timeline of clips for Shotstack
    """
    safe_runway = filter_reference_files(runway_outputs)
    safe_stock = filter_reference_files(stock_assets)
    safe_content = filter_reference_files(content_files)
    from context_engine import call_ai, SYSTEM_GUARDRAILS
    
    prioritized_assets = prioritize_content_files(safe_content, safe_runway, safe_stock)
    
    assets_context = {
        "runway_outputs": [{"id": i, "url": r.get("url"), "duration": r.get("duration"), "source": "runway"} 
                          for i, r in enumerate(safe_runway)],
        "stock_assets": [{"id": s.get("id"), "url": s.get("url"), "type": s.get("type"), "source": "stock"} 
                        for s in safe_stock[:10]],
        "content_files": [{"id": c.get("id"), "url": c.get("url"), "type": c.get("type"), "source": "content", "priority": "HIGHEST"} 
                         for c in safe_content]
    }
    
    print(f"[Shotstack Timeline] Assets after guardrails: {len(safe_content)} content, {len(safe_runway)} runway, {len(safe_stock)} stock")
    
    prompt = f"""Generate precise Shotstack timeline instructions.

This is SURGICAL ASSEMBLY - every clip needs exact placement.

VIBE PROFILE:
- Cut Rhythm: {vibe.cut_rhythm}
- Energy Level: {vibe.energy_level}
- Pacing: {vibe.pacing}
- Color Feel: {vibe.color_feel}

AVAILABLE ASSETS:
{json.dumps(assets_context, indent=2)}

AUDIO TRACK: {json.dumps(audio_track) if audio_track else "None specified"}
CAPTIONS: {len(captions or [])} caption segments

PRIORITY ORDER:
1. User's content files (if any) - HIGHEST priority
2. Runway generated videos - Core visual layer
3. Stock assets - Supporting/blended

TRANSITION RULES based on vibe:
- {vibe.cut_rhythm} rhythm = {"quick cuts, 0.1s transitions" if vibe.cut_rhythm == "punchy" else "smooth 0.3-0.5s transitions" if vibe.cut_rhythm == "flowing" else "clean cuts, minimal transitions"}
- Energy {vibe.energy_level} = {"fast pacing, 2-4s per clip" if vibe.energy_level > 0.7 else "moderate 4-6s per clip" if vibe.energy_level > 0.4 else "slow 6-10s per clip"}

Generate a timeline where each clip has:
- asset_url: Which asset to use
- asset_type: video/image/audio/text
- start_time: Exact start in seconds
- duration: Exact duration in seconds
- track: Layer number (0=bottom, higher=overlay)
- position_x: 0.0-1.0 horizontal (0.5=center)
- position_y: 0.0-1.0 vertical (0.5=center)
- scale: Size multiplier (1.0=100%)
- opacity: 0.0-1.0
- transition_in: fade/slide/zoom/none
- transition_out: fade/slide/zoom/none
- effects: [array of effects to apply]

Output JSON:
{{
    "timeline": [
        {{
            "asset_url": "url_here",
            "asset_type": "video",
            "start_time": 0.0,
            "duration": 5.0,
            "track": 0,
            "position_x": 0.5,
            "position_y": 0.5,
            "scale": 1.0,
            "opacity": 1.0,
            "transition_in": "fade",
            "transition_out": "fade",
            "effects": ["color_grade_cinematic"]
        }}
    ],
    "total_duration": 30.0,
    "output_settings": {{
        "resolution": "1080x1920",
        "fps": 30,
        "format": "mp4"
    }}
}}"""

    result = call_ai(prompt, SYSTEM_GUARDRAILS, json_output=True, max_tokens=2000)
    
    clips = []
    if result and result.get("timeline"):
        for clip in result["timeline"]:
            clips.append(ShotStackClip(
                asset_url=clip.get("asset_url", ""),
                asset_type=clip.get("asset_type", "video"),
                start_time=float(clip.get("start_time", 0)),
                duration=float(clip.get("duration", 5)),
                track=int(clip.get("track", 0)),
                position_x=float(clip.get("position_x", 0.5)),
                position_y=float(clip.get("position_y", 0.5)),
                scale=float(clip.get("scale", 1.0)),
                opacity=float(clip.get("opacity", 1.0)),
                transition_in=clip.get("transition_in", "none"),
                transition_out=clip.get("transition_out", "none"),
                effects=clip.get("effects", [])
            ))
    
    return clips


def build_shotstack_json(clips: List[ShotStackClip], output_settings: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Convert our timeline clips to Shotstack's exact JSON format.
    Uses official Shotstack Edit API v1 schema.
    
    Args:
        clips: List of ShotStackClip instructions
        output_settings: Resolution, FPS, format settings
        
    Returns:
        Complete Shotstack Edit API payload matching official schema
    """
    settings = output_settings or {
        "resolution": "1080",
        "format": "mp4",
        "aspectRatio": "9:16"
    }
    
    tracks_dict: Dict[int, List[Dict[str, Any]]] = {}
    for clip in clips:
        track_num = clip.track
        if track_num not in tracks_dict:
            tracks_dict[track_num] = []
        
        asset_data: Dict[str, Any] = {
            "type": clip.asset_type,
            "src": clip.asset_url
        }
        
        if clip.asset_type == "video":
            asset_data["volume"] = 1.0
        
        clip_data: Dict[str, Any] = {
            "asset": asset_data,
            "start": clip.start_time,
            "length": clip.duration
        }
        
        if clip.scale != 1.0:
            clip_data["scale"] = clip.scale
        
        if clip.opacity != 1.0:
            clip_data["opacity"] = clip.opacity
        
        position_x_offset = (clip.position_x - 0.5) * 2
        position_y_offset = (clip.position_y - 0.5) * -2
        if abs(position_x_offset) > 0.01 or abs(position_y_offset) > 0.01:
            clip_data["offset"] = {
                "x": round(position_x_offset, 3),
                "y": round(position_y_offset, 3)
            }
        else:
            clip_data["position"] = "center"
        
        if clip.transition_in != "none" or clip.transition_out != "none":
            transition: Dict[str, str] = {}
            if clip.transition_in != "none":
                transition["in"] = clip.transition_in
            if clip.transition_out != "none":
                transition["out"] = clip.transition_out
            if transition:
                clip_data["transition"] = transition
        
        effect_map = {
            "zoomIn": "zoomIn",
            "zoomOut": "zoomOut",
            "slideLeft": "slideLeft",
            "slideRight": "slideRight",
            "slideUp": "slideUp",
            "slideDown": "slideDown",
            "color_grade_cinematic": "boost"
        }
        
        if clip.effects:
            mapped_effect = effect_map.get(clip.effects[0], clip.effects[0])
            if mapped_effect in ["zoomIn", "zoomOut", "slideLeft", "slideRight", "slideUp", "slideDown"]:
                clip_data["effect"] = mapped_effect
        
        tracks_dict[track_num].append(clip_data)
    
    sorted_tracks = []
    for track_num in sorted(tracks_dict.keys(), reverse=True):
        sorted_tracks.append({"clips": tracks_dict[track_num]})
    
    return {
        "timeline": {
            "background": "#000000",
            "tracks": sorted_tracks
        },
        "output": {
            "format": settings.get("format", "mp4"),
            "resolution": settings.get("resolution", "1080"),
            "aspectRatio": settings.get("aspectRatio", "9:16")
        }
    }


def submit_to_shotstack(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    Submit render job to Shotstack API.
    
    Args:
        payload: Complete Shotstack Edit API payload
        
    Returns:
        Response with render ID and status URL
    """
    if not SHOTSTACK_API_KEY:
        return {"error": "SHOTSTACK_API_KEY not configured", "success": False}
    
    headers = {
        "x-api-key": SHOTSTACK_API_KEY,
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.post(
            f"{SHOTSTACK_BASE_URL}/render",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        if response.status_code == 201:
            data = response.json()
            return {
                "success": True,
                "render_id": data.get("response", {}).get("id"),
                "status_url": f"{SHOTSTACK_BASE_URL}/render/{data.get('response', {}).get('id')}",
                "message": data.get("response", {}).get("message")
            }
        else:
            return {
                "success": False,
                "error": response.text,
                "status_code": response.status_code
            }
    except Exception as e:
        return {"success": False, "error": str(e)}


def check_shotstack_status(render_id: str) -> Dict[str, Any]:
    """
    Check status of a Shotstack render job.
    
    Args:
        render_id: The render job ID
        
    Returns:
        Status and output URL if complete
    """
    if not SHOTSTACK_API_KEY:
        return {"error": "SHOTSTACK_API_KEY not configured"}
    
    headers = {"x-api-key": SHOTSTACK_API_KEY}
    
    try:
        response = requests.get(
            f"{SHOTSTACK_BASE_URL}/render/{render_id}",
            headers=headers,
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            status_data = data.get("response", {})
            return {
                "status": status_data.get("status"),
                "progress": status_data.get("progress", 0),
                "url": status_data.get("url"),
                "render_time": status_data.get("renderTime")
            }
        else:
            return {"error": response.text}
    except Exception as e:
        return {"error": str(e)}


def validate_file_types(
    reference_files: Optional[List[Dict[str, Any]]],
    content_files: Optional[List[Dict[str, Any]]]
) -> tuple:
    """
    GUARDRAIL: Validate and tag file types at orchestration entry point.
    Ensures reference files are tagged and cannot leak into content pipelines.
    
    Args:
        reference_files: Files intended for vibe extraction only
        content_files: Files intended for integration
        
    Returns:
        Tuple of (validated_references, validated_content)
    """
    validated_refs = []
    if reference_files:
        for f in reference_files:
            f["file_type"] = FileType.REFERENCE.value
            f["_integration_blocked"] = True
            validated_refs.append(f)
        print(f"[GUARDRAIL] Tagged {len(validated_refs)} reference files (blocked from integration)")
    
    validated_content = []
    if content_files:
        for f in content_files:
            if f.get("file_type") == FileType.REFERENCE.value or f.get("_integration_blocked"):
                print(f"[GUARDRAIL] BLOCKED: Reference file attempted to enter content pipeline: {f.get('name', 'unknown')}")
                continue
            f["file_type"] = FileType.CONTENT.value
            validated_content.append(f)
        print(f"[GUARDRAIL] Validated {len(validated_content)} content files for integration")
    
    return validated_refs, validated_content


def create_orchestration_plan(
    user_input: str,
    reference_files: Optional[List[Dict[str, Any]]] = None,
    content_files: Optional[List[Dict[str, Any]]] = None,
    script_segments: Optional[List[Dict[str, Any]]] = None
) -> OrchestrationPlan:
    """
    Create complete orchestration plan for all APIs working in conjunction.
    
    GUARDRAILS ENFORCED:
    1. Reference files are validated and blocked from integration at entry
    2. Content files are validated before processing
    3. File type tags are enforced throughout the pipeline
    
    This is the main entry point that:
    1. Validates and tags all input files
    2. Extracts vibe from reference files (NOT for integration)
    3. Researches topic presentation preferences
    4. Generates Runway instructions
    5. Prepares Shotstack timeline
    
    Args:
        user_input: User's content/idea
        reference_files: Files for vibe extraction ONLY (will be blocked from integration)
        content_files: Files to actually integrate (will be prioritized)
        script_segments: Script with timing
        
    Returns:
        Complete orchestration plan
    """
    print("[Orchestration] Starting surgical plan creation...")
    
    validated_refs, validated_content = validate_file_types(reference_files, content_files)
    
    reference = validated_refs[0] if validated_refs else None
    vibe = extract_vibe_from_reference(reference, user_input)
    print(f"[Orchestration] Vibe extracted: {vibe.mood}, energy={vibe.energy_level}")
    
    topic = user_input[:100]
    presentation = research_topic_presentation(topic, vibe)
    print(f"[Orchestration] Presentation research complete")
    
    runway_instructions = generate_runway_instructions(
        vibe=vibe,
        presentation=presentation,
        script_segments=script_segments or [],
        stock_assets=[]
    )
    print(f"[Orchestration] Generated {len(runway_instructions)} Runway instructions")
    
    total_duration = sum(r.duration for r in runway_instructions) if runway_instructions else 30.0
    estimated_cost = (total_duration / 30.0) * 5.10
    
    return OrchestrationPlan(
        vibe_profile=vibe,
        runway_instructions=runway_instructions,
        stock_queries=presentation.get("broll_patterns", []),
        shotstack_timeline=[],
        total_duration=total_duration,
        estimated_cost=round(estimated_cost, 2)
    )


def execute_orchestration(
    plan: OrchestrationPlan,
    quality_tier: QualityTier = QualityTier.GOOD,
    source_images: Optional[List[Dict[str, Any]]] = None,
    content_files: Optional[List[Dict[str, Any]]] = None,
    wait_for_completion: bool = True
) -> Dict[str, Any]:
    """
    Execute the orchestration plan: Runway -> Stock -> Shotstack.
    
    SURGICAL IMPLEMENTATION:
    1. Generate visuals via Runway API with selected quality tier
    2. Fetch stock assets based on orchestration queries
    3. Transform Runway outputs + stock into Shotstack timeline
    4. Submit to Shotstack for final assembly
    
    Args:
        plan: The complete orchestration plan
        quality_tier: Quality tier for Runway model selection
        source_images: Base images for image-to-video generation
        wait_for_completion: Whether to poll until all tasks complete
        
    Returns:
        Execution result with task IDs or final video URL
    """
    results: Dict[str, Any] = {
        "status": "processing",
        "quality_tier": quality_tier.value,
        "runway_tasks": [],
        "runway_outputs": [],
        "stock_assets": [],
        "shotstack_result": None,
        "final_video_url": None,
        "api_status": {
            "runway": "ready" if RUNWAY_API_KEY else "missing_api_key",
            "shotstack": "ready" if SHOTSTACK_API_KEY else "missing_api_key"
        },
        "errors": []
    }
    
    tier_info = get_quality_tier_info(quality_tier)
    print(f"[Execute] Using quality tier: {tier_info['name']} (model={tier_info['model']}, ${tier_info['price_per_30s']}/30s)")
    
    if not RUNWAY_API_KEY:
        results["errors"].append("RUNWAY_API_KEY not configured - cannot generate videos")
        results["user_message"] = "Video generation is not available at the moment."
        print("[Execute] ERROR: RUNWAY_API_KEY missing")
    else:
        print(f"[Execute] Starting Runway generation for {len(plan.runway_instructions)} scenes via queue...")
        
        runway_requests = []
        for i, instr in enumerate(plan.runway_instructions):
            source_image = None
            if source_images and len(source_images) > i:
                source_image = source_images[i].get("url")
            elif source_images and len(source_images) > 0:
                source_image = source_images[0].get("url")
            
            if not source_image:
                results["runway_tasks"].append({
                    "scene_id": instr.scene_id,
                    "status": "skipped",
                    "reason": "No source image provided"
                })
                continue
            
            runway_requests.append({
                "scene_id": instr.scene_id,
                "prompt_image": source_image,
                "prompt_text": instr.prompt,
                "duration": min(int(instr.duration), 10),
                "ratio": "9:16"
            })
        
        if runway_requests:
            def progress_callback(current, total, message):
                results["progress"] = {"current": current, "total": total, "message": message}
                print(f"[Execute] {message}")
            
            queue_results = RUNWAY_QUEUE.process_batch(
                requests=runway_requests,
                quality_tier=quality_tier,
                on_progress=progress_callback if wait_for_completion else None
            )
            
            for i, runway_result in enumerate(queue_results):
                scene_id = runway_requests[i]["scene_id"]
                
                if runway_result.get("success"):
                    results["runway_tasks"].append({
                        "scene_id": scene_id,
                        "task_id": runway_result.get("task_id"),
                        "status": runway_result.get("status"),
                        "output_url": runway_result.get("output_url")
                    })
                    
                    if runway_result.get("output_url"):
                        results["runway_outputs"].append({
                            "scene_id": scene_id,
                            "url": runway_result.get("output_url"),
                            "duration": runway_requests[i]["duration"],
                            "type": "video",
                            "source": "runway"
                        })
                else:
                    user_msg = runway_result.get("user_message", runway_result.get("error", "Generation failed"))
                    results["runway_tasks"].append({
                        "scene_id": scene_id,
                        "status": "failed",
                        "error": runway_result.get("error"),
                        "error_code": runway_result.get("error_code"),
                        "user_message": user_msg
                    })
                    results["errors"].append(f"Scene {scene_id}: {user_msg}")
        
        print(f"[Execute] Queue complete: {len(results['runway_outputs'])} videos generated")
    
    print("[Execute] Fetching stock assets...")
    for query in plan.stock_queries:
        query_str = query if isinstance(query, str) else str(query)
        results["stock_assets"].append({
            "query": query_str,
            "status": "pending",
            "url": None
        })
    
    validated_content = filter_reference_files(content_files or [])
    has_content = bool(results["runway_outputs"] or validated_content or any(a.get("url") for a in results["stock_assets"]))
    
    if has_content:
        if not SHOTSTACK_API_KEY:
            results["errors"].append("SHOTSTACK_API_KEY not configured - cannot assemble video")
            print("[Execute] ERROR: SHOTSTACK_API_KEY missing")
        else:
            print("[Execute] Building Shotstack timeline...")
            
            clips = transform_to_shotstack_clips(
                runway_outputs=results["runway_outputs"],
                stock_assets=[a for a in results["stock_assets"] if a.get("url")],
                vibe=plan.vibe_profile,
                content_files=validated_content
            )
            
            if clips:
                shotstack_payload = build_shotstack_json(clips)
                
                print("[Execute] Submitting to Shotstack...")
                shotstack_result = submit_to_shotstack(shotstack_payload)
                results["shotstack_result"] = shotstack_result
                
                if shotstack_result.get("success"):
                    print(f"[Execute] Shotstack render started: {shotstack_result.get('render_id')}")
                    
                    if wait_for_completion and shotstack_result.get("render_id"):
                        final_status = shotstack_wait_for_completion(
                            str(shotstack_result.get("render_id"))
                        )
                        results["shotstack_result"].update(final_status)
                        if final_status.get("url"):
                            results["final_video_url"] = final_status.get("url")
                            results["status"] = "completed"
                else:
                    results["errors"].append(f"Shotstack error: {shotstack_result.get('error')}")
    
    if not results["errors"]:
        if results["final_video_url"]:
            results["status"] = "completed"
        elif results["runway_tasks"] or results.get("shotstack_result", {}).get("render_id"):
            results["status"] = "processing"
        else:
            results["status"] = "no_content"
    else:
        results["status"] = "partial_failure" if results["runway_outputs"] else "failed"
    
    print(f"[Execute] Final status: {results['status']}")
    return results


def transform_to_shotstack_clips(
    runway_outputs: List[Dict[str, Any]],
    stock_assets: List[Dict[str, Any]],
    vibe: VibeProfile,
    content_files: Optional[List[Dict[str, Any]]] = None
) -> List[ShotStackClip]:
    """
    Transform Runway outputs and stock assets into Shotstack timeline clips.
    This is the surgical transformation layer that maps API outputs to timeline.
    
    Priority order:
    1. Content files (user uploads) - HIGHEST
    2. Runway outputs (AI generated)
    3. Stock assets (gap filling)
    
    Args:
        runway_outputs: List of Runway video outputs with URLs
        stock_assets: List of stock assets with URLs
        vibe: Vibe profile for styling decisions
        content_files: Optional user content files
        
    Returns:
        List of ShotStackClip objects for timeline
    """
    clips = []
    current_time = 0.0
    track = 0
    
    transition = "fade" if vibe.cut_rhythm in ["flowing", "steady"] else "none"
    
    if content_files:
        for cf in filter_reference_files(content_files):
            if cf.get("url"):
                duration = cf.get("duration", 5.0)
                clips.append(ShotStackClip(
                    asset_url=cf["url"],
                    asset_type=cf.get("type", "video"),
                    start_time=current_time,
                    duration=duration,
                    track=track,
                    position_x=0.5,
                    position_y=0.5,
                    scale=1.0,
                    opacity=1.0,
                    transition_in=transition,
                    transition_out=transition,
                    effects=[]
                ))
                current_time += duration
    
    for ro in runway_outputs:
        if ro.get("url"):
            duration = ro.get("duration", 5.0)
            clips.append(ShotStackClip(
                asset_url=ro["url"],
                asset_type="video",
                start_time=current_time,
                duration=duration,
                track=track,
                position_x=0.5,
                position_y=0.5,
                scale=1.0,
                opacity=1.0,
                transition_in=transition,
                transition_out=transition,
                effects=[]
            ))
            current_time += duration
    
    for sa in stock_assets:
        if sa.get("url"):
            duration = sa.get("duration", 3.0)
            clips.append(ShotStackClip(
                asset_url=sa["url"],
                asset_type=sa.get("type", "video"),
                start_time=current_time,
                duration=duration,
                track=track,
                position_x=0.5,
                position_y=0.5,
                scale=1.0,
                opacity=1.0,
                transition_in=transition,
                transition_out=transition,
                effects=[]
            ))
            current_time += duration
    
    print(f"[Transform] Created {len(clips)} clips, total duration: {current_time}s")
    return clips


def shotstack_wait_for_completion(
    render_id: str,
    max_wait_seconds: int = 300,
    poll_interval: int = 5
) -> Dict[str, Any]:
    """
    Poll Shotstack render until completion or timeout.
    
    Args:
        render_id: The render job ID
        max_wait_seconds: Maximum time to wait
        poll_interval: Seconds between polls
        
    Returns:
        Final status with URL if complete
    """
    import time
    
    elapsed = 0
    while elapsed < max_wait_seconds:
        status = check_shotstack_status(render_id)
        
        print(f"[Shotstack] Render {render_id[:8]}... status={status.get('status')}")
        
        if status.get("status") == "done":
            return {
                "status": "done",
                "url": status.get("url"),
                "render_time": status.get("render_time")
            }
        elif status.get("status") == "failed":
            return {
                "status": "failed",
                "error": status.get("error", "Render failed")
            }
        elif status.get("error"):
            return status
        
        time.sleep(poll_interval)
        elapsed += poll_interval
    
    return {"status": "timeout", "error": f"Render timed out after {max_wait_seconds}s"}
