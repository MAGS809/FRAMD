"""
Remix Engine: Vibe-Based Video Creation with Surgical API Orchestration

This module handles the complete Remix pipeline:
1. Vibe Extraction - Analyze reference files for mood/energy/pacing (NOT for integration)
2. Topic Research - How internet audiences prefer content about the topic
3. Runway API - Generate/transform visuals based on vibe
4. Stock Integration - Fill gaps, blended creatively by Runway
5. Shotstack API - Precise JSON instructions for final assembly

File Type Distinction:
- Reference Files: Vibe extraction ONLY, never integrated into final video
- Content Files: User's actual content to prioritize in final video
- Stock Assets: Fill gaps, transformed through Runway for cohesion
- Runway Output: AI-generated scenes forming the visual core
"""

import os
import json
import requests
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, asdict
from enum import Enum

class FileType(Enum):
    REFERENCE = "reference"  # For vibe extraction only
    CONTENT = "content"      # To be integrated into final video
    STOCK = "stock"          # From stock libraries
    RUNWAY = "runway"        # AI-generated by Runway
    GENERATED = "generated"  # DALL-E or other AI images


@dataclass
class VibeProfile:
    """Extracted vibe characteristics from reference material."""
    mood: str           # calm, energetic, provocative, mysterious, etc.
    energy_level: float # 0.0 (slow/meditative) to 1.0 (high-energy)
    pacing: str         # slow, moderate, fast, variable
    color_feel: str     # warm, cool, neutral, vibrant, muted
    cut_rhythm: str     # steady, punchy, flowing, chaotic
    audio_vibe: str     # ambient, upbeat, dramatic, minimal
    visual_density: str # sparse, balanced, dense
    emotional_arc: str  # build, release, constant, wave


@dataclass 
class ShotStackClip:
    """Precise clip instruction for Shotstack API."""
    asset_url: str
    asset_type: str     # video, image, audio, text
    start_time: float   # seconds
    duration: float     # seconds
    track: int          # layer number (0 = bottom)
    position_x: float   # 0.0 to 1.0
    position_y: float   # 0.0 to 1.0
    scale: float        # 1.0 = 100%
    opacity: float      # 0.0 to 1.0
    transition_in: str  # fade, slide, zoom, none
    transition_out: str
    effects: List[str]  # color_grade, blur, etc.


@dataclass
class RunwayInstruction:
    """Precise instruction for Runway API generation."""
    scene_id: int
    generation_type: str  # image_to_video, text_to_video
    prompt: str
    duration: float       # seconds
    style_reference: str  # style description from vibe
    motion_guidance: str  # how motion should flow
    blend_with: List[str] # stock asset IDs to blend


@dataclass
class OrchestrationPlan:
    """Complete surgical plan for all APIs working in conjunction."""
    vibe_profile: VibeProfile
    runway_instructions: List[RunwayInstruction]
    stock_queries: List[Dict[str, Any]]
    shotstack_timeline: List[ShotStackClip]
    total_duration: float
    estimated_cost: float


# API Configuration
RUNWAY_API_KEY = os.environ.get("RUNWAY_API_KEY")
SHOTSTACK_API_KEY = os.environ.get("SHOTSTACK_API_KEY")
SHOTSTACK_ENV = os.environ.get("SHOTSTACK_ENV", "stage")  # stage or v1

SHOTSTACK_BASE_URL = f"https://api.shotstack.io/{SHOTSTACK_ENV}"


def extract_vibe_from_reference(
    reference_file: Optional[Dict[str, Any]] = None,
    user_description: Optional[str] = None
) -> VibeProfile:
    """
    Extract vibe characteristics from a reference file.
    This is for DIRECTION ONLY - the reference is never integrated into the final video.
    
    Args:
        reference_file: Dict with 'url', 'type', 'name' of reference material
        user_description: Optional text description of desired vibe
        
    Returns:
        VibeProfile with extracted characteristics
    """
    from context_engine import call_ai, SYSTEM_GUARDRAILS
    
    file_context = ""
    if reference_file:
        file_context = f"""
REFERENCE FILE (for vibe extraction only, NOT for integration):
- Name: {reference_file.get('name', 'unknown')}
- Type: {reference_file.get('type', 'video')}
- URL: {reference_file.get('url', '')}
"""
    
    user_context = ""
    if user_description:
        user_context = f"\nUSER'S VIBE DESCRIPTION: {user_description}"
    
    prompt = f"""Analyze this reference material to extract VIBE characteristics only.

IMPORTANT: This reference is for DIRECTION/MOOD extraction. It will NOT be integrated into the final video.
The goal is to understand the energy, pacing, and feel so we can CREATE NEW content that matches this vibe.
{file_context}{user_context}

Extract the following vibe characteristics:

1. MOOD: What emotional tone does this convey? (calm, energetic, provocative, mysterious, inspirational, urgent, playful, serious, intimate, epic)

2. ENERGY_LEVEL: Rate 0.0 to 1.0
   - 0.0-0.3: Slow, meditative, contemplative
   - 0.4-0.6: Moderate, balanced, conversational
   - 0.7-1.0: High-energy, fast-paced, intense

3. PACING: How do cuts and transitions flow?
   - slow: Long takes, gentle transitions
   - moderate: Balanced rhythm
   - fast: Quick cuts, dynamic
   - variable: Builds and releases

4. COLOR_FEEL: Overall color palette feeling
   - warm: Golden, orange, cozy
   - cool: Blue, teal, clinical
   - neutral: Balanced, natural
   - vibrant: Saturated, punchy
   - muted: Desaturated, film-like

5. CUT_RHYTHM: How edits land
   - steady: Predictable, consistent
   - punchy: Sharp, impactful
   - flowing: Smooth, continuous
   - chaotic: Unpredictable, energetic

6. AUDIO_VIBE: What audio would complement
   - ambient: Atmospheric, background
   - upbeat: Energetic, driving
   - dramatic: Cinematic, tension
   - minimal: Sparse, focused

7. VISUAL_DENSITY: How much is happening on screen
   - sparse: Clean, minimal elements
   - balanced: Moderate complexity
   - dense: Layered, busy

8. EMOTIONAL_ARC: How emotion flows through
   - build: Starts quiet, grows
   - release: Starts intense, calms
   - constant: Maintains level
   - wave: Rises and falls

Output JSON:
{{
    "mood": "string",
    "energy_level": 0.0-1.0,
    "pacing": "slow|moderate|fast|variable",
    "color_feel": "warm|cool|neutral|vibrant|muted",
    "cut_rhythm": "steady|punchy|flowing|chaotic",
    "audio_vibe": "ambient|upbeat|dramatic|minimal",
    "visual_density": "sparse|balanced|dense",
    "emotional_arc": "build|release|constant|wave"
}}"""

    result = call_ai(prompt, SYSTEM_GUARDRAILS, json_output=True, max_tokens=512)
    
    if result:
        return VibeProfile(
            mood=result.get("mood", "balanced"),
            energy_level=float(result.get("energy_level", 0.5)),
            pacing=result.get("pacing", "moderate"),
            color_feel=result.get("color_feel", "neutral"),
            cut_rhythm=result.get("cut_rhythm", "steady"),
            audio_vibe=result.get("audio_vibe", "ambient"),
            visual_density=result.get("visual_density", "balanced"),
            emotional_arc=result.get("emotional_arc", "constant")
        )
    
    return VibeProfile(
        mood="balanced",
        energy_level=0.5,
        pacing="moderate",
        color_feel="neutral",
        cut_rhythm="steady",
        audio_vibe="ambient",
        visual_density="balanced",
        emotional_arc="constant"
    )


def research_topic_presentation(topic: str, vibe: VibeProfile) -> Dict[str, Any]:
    """
    Research how the internet prefers to consume content about this topic.
    Combines vibe direction with audience preference research.
    
    Args:
        topic: The user's content topic
        vibe: Extracted vibe profile to maintain
        
    Returns:
        Presentation preferences that inform visual decisions
    """
    from context_engine import call_ai, SYSTEM_GUARDRAILS
    from duckduckgo_search import DDGS
    
    search_results = []
    try:
        with DDGS() as ddgs:
            queries = [
                f"{topic} viral video format 2025",
                f"{topic} TikTok Reels what works",
                f"how to present {topic} short form video"
            ]
            for query in queries:
                results = list(ddgs.text(query, max_results=3))
                for r in results:
                    search_results.append({
                        "title": r.get("title", ""),
                        "snippet": r.get("body", ""),
                        "source": r.get("href", "")
                    })
    except Exception as e:
        print(f"[TopicResearch] Search error: {e}")
    
    search_context = "\n".join([
        f"- {r['title']}: {r['snippet'][:150]}"
        for r in search_results[:8]
    ])
    
    prompt = f"""Research how audiences prefer to consume content about "{topic}".

VIBE DIRECTION (must maintain):
- Mood: {vibe.mood}
- Energy: {vibe.energy_level}
- Pacing: {vibe.pacing}

WEB RESEARCH:
{search_context if search_context else "No specific research available - use general short-form best practices"}

Based on this, determine:

1. VISUAL APPROACH: What types of visuals work for this topic?
2. HOOK STYLE: What opening grabs attention for this topic?
3. INFORMATION DENSITY: How much detail per second?
4. B-ROLL PATTERNS: What supporting visuals are expected?
5. TEXT USAGE: How much on-screen text is normal?
6. PACING PREFERENCES: Topic-specific pacing notes

IMPORTANT: Recommendations must WORK WITH the vibe direction, not override it.
The vibe sets the feel. The research sets the format.

Output JSON:
{{
    "visual_approach": "what visual style works for this topic",
    "hook_style": "how to open videos about this topic",
    "information_density": "low|medium|high",
    "broll_patterns": ["type of b-roll 1", "type of b-roll 2"],
    "text_usage": "minimal|moderate|heavy",
    "pacing_notes": "specific pacing recommendations",
    "avoid": ["what doesn't work for this topic"],
    "successful_patterns": ["pattern that works", "another pattern"]
}}"""

    result = call_ai(prompt, SYSTEM_GUARDRAILS, json_output=True, max_tokens=768)
    
    return result if result else {
        "visual_approach": "Clean, professional visuals",
        "hook_style": "Direct statement or question",
        "information_density": "medium",
        "broll_patterns": ["relevant footage", "supporting imagery"],
        "text_usage": "moderate",
        "pacing_notes": "Match content complexity",
        "avoid": [],
        "successful_patterns": []
    }


def generate_runway_instructions(
    vibe: VibeProfile,
    presentation: Dict[str, Any],
    script_segments: Optional[List[Dict[str, Any]]] = None,
    stock_assets: Optional[List[Dict[str, Any]]] = None
) -> List[RunwayInstruction]:
    """
    Generate precise Runway API instructions based on vibe and presentation research.
    Runway will generate/transform visuals and blend stock footage.
    
    Args:
        vibe: Extracted vibe profile
        presentation: Topic presentation research
        script_segments: List of script segments with timing
        stock_assets: Stock assets to potentially blend
        
    Returns:
        List of precise Runway instructions
    """
    from context_engine import call_ai, SYSTEM_GUARDRAILS
    
    stock_context = ""
    if stock_assets:
        stock_context = f"""
STOCK ASSETS AVAILABLE FOR BLENDING:
{json.dumps([{'id': s.get('id'), 'description': s.get('description', '')[:50]} for s in stock_assets[:5]], indent=2)}
"""
    
    prompt = f"""Generate precise Runway API instructions for video generation.

VIBE PROFILE:
- Mood: {vibe.mood}
- Energy: {vibe.energy_level}
- Pacing: {vibe.pacing}
- Color Feel: {vibe.color_feel}
- Cut Rhythm: {vibe.cut_rhythm}

PRESENTATION RESEARCH:
{json.dumps(presentation, indent=2)}

SCRIPT SEGMENTS:
{json.dumps(script_segments[:10], indent=2)}
{stock_context}

Generate Runway instructions for each scene. Each instruction must be:
1. PRECISE - Exact prompt that Runway can execute
2. VIBE-ALIGNED - Matches the mood and energy
3. COHESIVE - Works with other scenes as a whole
4. STOCK-AWARE - Note which stock assets to blend if applicable

For each scene, create:
- generation_type: "image_to_video" (from stock/DALL-E base) or "text_to_video" (pure generation)
- prompt: Detailed visual description for Runway
- duration: Exact seconds
- style_reference: How this matches the vibe
- motion_guidance: How motion should flow (camera movement, subject movement)
- blend_with: IDs of stock assets to incorporate (empty if pure generation)

Output JSON:
{{
    "runway_instructions": [
        {{
            "scene_id": 1,
            "generation_type": "image_to_video",
            "prompt": "Detailed Runway prompt here",
            "duration": 5.0,
            "style_reference": "How this matches {vibe.mood} mood",
            "motion_guidance": "Slow push in, subject remains centered",
            "blend_with": []
        }}
    ],
    "total_runway_seconds": 30,
    "style_notes": "Overall style coherence notes"
}}"""

    result = call_ai(prompt, SYSTEM_GUARDRAILS, json_output=True, max_tokens=1500)
    
    instructions = []
    if result and result.get("runway_instructions"):
        for instr in result["runway_instructions"]:
            instructions.append(RunwayInstruction(
                scene_id=instr.get("scene_id", 0),
                generation_type=instr.get("generation_type", "text_to_video"),
                prompt=instr.get("prompt", ""),
                duration=float(instr.get("duration", 5.0)),
                style_reference=instr.get("style_reference", ""),
                motion_guidance=instr.get("motion_guidance", ""),
                blend_with=instr.get("blend_with", [])
            ))
    
    return instructions


def filter_reference_files(files: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    """
    GUARDRAIL: Filter out reference files from any asset list.
    Reference files are for vibe extraction ONLY and must never enter the timeline.
    
    Args:
        files: List of files that may include reference files
        
    Returns:
        List with reference files removed
    """
    if not files:
        return []
    
    filtered = []
    for f in files:
        file_type = f.get("file_type", f.get("type", "content"))
        if file_type == FileType.REFERENCE.value or file_type == "reference":
            print(f"[GUARDRAIL] Excluding reference file from timeline: {f.get('name', 'unknown')}")
            continue
        filtered.append(f)
    
    return filtered


def prioritize_content_files(
    content_files: List[Dict[str, Any]],
    runway_outputs: List[Dict[str, Any]],
    stock_assets: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    GUARDRAIL: Ensure content files have priority in the asset order.
    Priority: Content files > Runway outputs > Stock assets
    
    Args:
        content_files: User's actual content to prioritize
        runway_outputs: Generated Runway videos
        stock_assets: Stock footage/images
        
    Returns:
        Ordered list with content files first
    """
    prioritized = []
    
    for f in content_files:
        f["priority"] = 1
        f["source"] = "content"
        prioritized.append(f)
    
    for f in runway_outputs:
        f["priority"] = 2
        f["source"] = "runway"
        prioritized.append(f)
    
    for f in stock_assets:
        f["priority"] = 3
        f["source"] = "stock"
        prioritized.append(f)
    
    return prioritized


def generate_shotstack_timeline(
    vibe: VibeProfile,
    runway_outputs: Optional[List[Dict[str, Any]]] = None,
    stock_assets: Optional[List[Dict[str, Any]]] = None,
    content_files: Optional[List[Dict[str, Any]]] = None,
    audio_track: Optional[Dict[str, Any]] = None,
    captions: Optional[List[Dict[str, Any]]] = None
) -> List[ShotStackClip]:
    """
    Generate precise Shotstack timeline with exact frame/layer/timing/position.
    This is the surgical assembly instruction set.
    
    GUARDRAILS APPLIED:
    1. Reference files are automatically filtered out
    2. Content files are prioritized over Runway and Stock
    3. Each asset is tagged with its source for traceability
    
    Args:
        vibe: Vibe profile for transition/effect decisions
        runway_outputs: Generated Runway video URLs
        stock_assets: Stock footage/images
        content_files: User's actual content to prioritize (NOT reference files)
        audio_track: Main audio
        captions: Caption data with timing
        
    Returns:
        Precise timeline of clips for Shotstack
    """
    safe_runway = filter_reference_files(runway_outputs)
    safe_stock = filter_reference_files(stock_assets)
    safe_content = filter_reference_files(content_files)
    from context_engine import call_ai, SYSTEM_GUARDRAILS
    
    prioritized_assets = prioritize_content_files(safe_content, safe_runway, safe_stock)
    
    assets_context = {
        "runway_outputs": [{"id": i, "url": r.get("url"), "duration": r.get("duration"), "source": "runway"} 
                          for i, r in enumerate(safe_runway)],
        "stock_assets": [{"id": s.get("id"), "url": s.get("url"), "type": s.get("type"), "source": "stock"} 
                        for s in safe_stock[:10]],
        "content_files": [{"id": c.get("id"), "url": c.get("url"), "type": c.get("type"), "source": "content", "priority": "HIGHEST"} 
                         for c in safe_content]
    }
    
    print(f"[Shotstack Timeline] Assets after guardrails: {len(safe_content)} content, {len(safe_runway)} runway, {len(safe_stock)} stock")
    
    prompt = f"""Generate precise Shotstack timeline instructions.

This is SURGICAL ASSEMBLY - every clip needs exact placement.

VIBE PROFILE:
- Cut Rhythm: {vibe.cut_rhythm}
- Energy Level: {vibe.energy_level}
- Pacing: {vibe.pacing}
- Color Feel: {vibe.color_feel}

AVAILABLE ASSETS:
{json.dumps(assets_context, indent=2)}

AUDIO TRACK: {json.dumps(audio_track) if audio_track else "None specified"}
CAPTIONS: {len(captions or [])} caption segments

PRIORITY ORDER:
1. User's content files (if any) - HIGHEST priority
2. Runway generated videos - Core visual layer
3. Stock assets - Supporting/blended

TRANSITION RULES based on vibe:
- {vibe.cut_rhythm} rhythm = {"quick cuts, 0.1s transitions" if vibe.cut_rhythm == "punchy" else "smooth 0.3-0.5s transitions" if vibe.cut_rhythm == "flowing" else "clean cuts, minimal transitions"}
- Energy {vibe.energy_level} = {"fast pacing, 2-4s per clip" if vibe.energy_level > 0.7 else "moderate 4-6s per clip" if vibe.energy_level > 0.4 else "slow 6-10s per clip"}

Generate a timeline where each clip has:
- asset_url: Which asset to use
- asset_type: video/image/audio/text
- start_time: Exact start in seconds
- duration: Exact duration in seconds
- track: Layer number (0=bottom, higher=overlay)
- position_x: 0.0-1.0 horizontal (0.5=center)
- position_y: 0.0-1.0 vertical (0.5=center)
- scale: Size multiplier (1.0=100%)
- opacity: 0.0-1.0
- transition_in: fade/slide/zoom/none
- transition_out: fade/slide/zoom/none
- effects: [array of effects to apply]

Output JSON:
{{
    "timeline": [
        {{
            "asset_url": "url_here",
            "asset_type": "video",
            "start_time": 0.0,
            "duration": 5.0,
            "track": 0,
            "position_x": 0.5,
            "position_y": 0.5,
            "scale": 1.0,
            "opacity": 1.0,
            "transition_in": "fade",
            "transition_out": "fade",
            "effects": ["color_grade_cinematic"]
        }}
    ],
    "total_duration": 30.0,
    "output_settings": {{
        "resolution": "1080x1920",
        "fps": 30,
        "format": "mp4"
    }}
}}"""

    result = call_ai(prompt, SYSTEM_GUARDRAILS, json_output=True, max_tokens=2000)
    
    clips = []
    if result and result.get("timeline"):
        for clip in result["timeline"]:
            clips.append(ShotStackClip(
                asset_url=clip.get("asset_url", ""),
                asset_type=clip.get("asset_type", "video"),
                start_time=float(clip.get("start_time", 0)),
                duration=float(clip.get("duration", 5)),
                track=int(clip.get("track", 0)),
                position_x=float(clip.get("position_x", 0.5)),
                position_y=float(clip.get("position_y", 0.5)),
                scale=float(clip.get("scale", 1.0)),
                opacity=float(clip.get("opacity", 1.0)),
                transition_in=clip.get("transition_in", "none"),
                transition_out=clip.get("transition_out", "none"),
                effects=clip.get("effects", [])
            ))
    
    return clips


def build_shotstack_json(clips: List[ShotStackClip], output_settings: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Convert our timeline clips to Shotstack's exact JSON format.
    Uses official Shotstack Edit API v1 schema.
    
    Args:
        clips: List of ShotStackClip instructions
        output_settings: Resolution, FPS, format settings
        
    Returns:
        Complete Shotstack Edit API payload matching official schema
    """
    settings = output_settings or {
        "resolution": "1080",
        "format": "mp4",
        "aspectRatio": "9:16"
    }
    
    tracks_dict: Dict[int, List[Dict[str, Any]]] = {}
    for clip in clips:
        track_num = clip.track
        if track_num not in tracks_dict:
            tracks_dict[track_num] = []
        
        asset_data: Dict[str, Any] = {
            "type": clip.asset_type,
            "src": clip.asset_url
        }
        
        if clip.asset_type == "video":
            asset_data["volume"] = 1.0
        
        clip_data: Dict[str, Any] = {
            "asset": asset_data,
            "start": clip.start_time,
            "length": clip.duration
        }
        
        if clip.scale != 1.0:
            clip_data["scale"] = clip.scale
        
        if clip.opacity != 1.0:
            clip_data["opacity"] = clip.opacity
        
        position_x_offset = (clip.position_x - 0.5) * 2
        position_y_offset = (clip.position_y - 0.5) * -2
        if abs(position_x_offset) > 0.01 or abs(position_y_offset) > 0.01:
            clip_data["offset"] = {
                "x": round(position_x_offset, 3),
                "y": round(position_y_offset, 3)
            }
        else:
            clip_data["position"] = "center"
        
        if clip.transition_in != "none" or clip.transition_out != "none":
            transition: Dict[str, str] = {}
            if clip.transition_in != "none":
                transition["in"] = clip.transition_in
            if clip.transition_out != "none":
                transition["out"] = clip.transition_out
            if transition:
                clip_data["transition"] = transition
        
        effect_map = {
            "zoomIn": "zoomIn",
            "zoomOut": "zoomOut",
            "slideLeft": "slideLeft",
            "slideRight": "slideRight",
            "slideUp": "slideUp",
            "slideDown": "slideDown",
            "color_grade_cinematic": "boost"
        }
        
        if clip.effects:
            mapped_effect = effect_map.get(clip.effects[0], clip.effects[0])
            if mapped_effect in ["zoomIn", "zoomOut", "slideLeft", "slideRight", "slideUp", "slideDown"]:
                clip_data["effect"] = mapped_effect
        
        tracks_dict[track_num].append(clip_data)
    
    sorted_tracks = []
    for track_num in sorted(tracks_dict.keys(), reverse=True):
        sorted_tracks.append({"clips": tracks_dict[track_num]})
    
    return {
        "timeline": {
            "background": "#000000",
            "tracks": sorted_tracks
        },
        "output": {
            "format": settings.get("format", "mp4"),
            "resolution": settings.get("resolution", "1080"),
            "aspectRatio": settings.get("aspectRatio", "9:16")
        }
    }


def submit_to_shotstack(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    Submit render job to Shotstack API.
    
    Args:
        payload: Complete Shotstack Edit API payload
        
    Returns:
        Response with render ID and status URL
    """
    if not SHOTSTACK_API_KEY:
        return {"error": "SHOTSTACK_API_KEY not configured", "success": False}
    
    headers = {
        "x-api-key": SHOTSTACK_API_KEY,
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.post(
            f"{SHOTSTACK_BASE_URL}/render",
            headers=headers,
            json=payload,
            timeout=30
        )
        
        if response.status_code == 201:
            data = response.json()
            return {
                "success": True,
                "render_id": data.get("response", {}).get("id"),
                "status_url": f"{SHOTSTACK_BASE_URL}/render/{data.get('response', {}).get('id')}",
                "message": data.get("response", {}).get("message")
            }
        else:
            return {
                "success": False,
                "error": response.text,
                "status_code": response.status_code
            }
    except Exception as e:
        return {"success": False, "error": str(e)}


def check_shotstack_status(render_id: str) -> Dict[str, Any]:
    """
    Check status of a Shotstack render job.
    
    Args:
        render_id: The render job ID
        
    Returns:
        Status and output URL if complete
    """
    if not SHOTSTACK_API_KEY:
        return {"error": "SHOTSTACK_API_KEY not configured"}
    
    headers = {"x-api-key": SHOTSTACK_API_KEY}
    
    try:
        response = requests.get(
            f"{SHOTSTACK_BASE_URL}/render/{render_id}",
            headers=headers,
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            status_data = data.get("response", {})
            return {
                "status": status_data.get("status"),
                "progress": status_data.get("progress", 0),
                "url": status_data.get("url"),
                "render_time": status_data.get("renderTime")
            }
        else:
            return {"error": response.text}
    except Exception as e:
        return {"error": str(e)}


def validate_file_types(
    reference_files: Optional[List[Dict[str, Any]]],
    content_files: Optional[List[Dict[str, Any]]]
) -> tuple:
    """
    GUARDRAIL: Validate and tag file types at orchestration entry point.
    Ensures reference files are tagged and cannot leak into content pipelines.
    
    Args:
        reference_files: Files intended for vibe extraction only
        content_files: Files intended for integration
        
    Returns:
        Tuple of (validated_references, validated_content)
    """
    validated_refs = []
    if reference_files:
        for f in reference_files:
            f["file_type"] = FileType.REFERENCE.value
            f["_integration_blocked"] = True
            validated_refs.append(f)
        print(f"[GUARDRAIL] Tagged {len(validated_refs)} reference files (blocked from integration)")
    
    validated_content = []
    if content_files:
        for f in content_files:
            if f.get("file_type") == FileType.REFERENCE.value or f.get("_integration_blocked"):
                print(f"[GUARDRAIL] BLOCKED: Reference file attempted to enter content pipeline: {f.get('name', 'unknown')}")
                continue
            f["file_type"] = FileType.CONTENT.value
            validated_content.append(f)
        print(f"[GUARDRAIL] Validated {len(validated_content)} content files for integration")
    
    return validated_refs, validated_content


def create_orchestration_plan(
    user_input: str,
    reference_files: Optional[List[Dict[str, Any]]] = None,
    content_files: Optional[List[Dict[str, Any]]] = None,
    script_segments: Optional[List[Dict[str, Any]]] = None
) -> OrchestrationPlan:
    """
    Create complete orchestration plan for all APIs working in conjunction.
    
    GUARDRAILS ENFORCED:
    1. Reference files are validated and blocked from integration at entry
    2. Content files are validated before processing
    3. File type tags are enforced throughout the pipeline
    
    This is the main entry point that:
    1. Validates and tags all input files
    2. Extracts vibe from reference files (NOT for integration)
    3. Researches topic presentation preferences
    4. Generates Runway instructions
    5. Prepares Shotstack timeline
    
    Args:
        user_input: User's content/idea
        reference_files: Files for vibe extraction ONLY (will be blocked from integration)
        content_files: Files to actually integrate (will be prioritized)
        script_segments: Script with timing
        
    Returns:
        Complete orchestration plan
    """
    print("[Orchestration] Starting surgical plan creation...")
    
    validated_refs, validated_content = validate_file_types(reference_files, content_files)
    
    reference = validated_refs[0] if validated_refs else None
    vibe = extract_vibe_from_reference(reference, user_input)
    print(f"[Orchestration] Vibe extracted: {vibe.mood}, energy={vibe.energy_level}")
    
    topic = user_input[:100]
    presentation = research_topic_presentation(topic, vibe)
    print(f"[Orchestration] Presentation research complete")
    
    runway_instructions = generate_runway_instructions(
        vibe=vibe,
        presentation=presentation,
        script_segments=script_segments or [],
        stock_assets=[]
    )
    print(f"[Orchestration] Generated {len(runway_instructions)} Runway instructions")
    
    total_duration = sum(r.duration for r in runway_instructions) if runway_instructions else 30.0
    estimated_cost = (total_duration / 30.0) * 5.10
    
    return OrchestrationPlan(
        vibe_profile=vibe,
        runway_instructions=runway_instructions,
        stock_queries=presentation.get("broll_patterns", []),
        shotstack_timeline=[],
        total_duration=total_duration,
        estimated_cost=round(estimated_cost, 2)
    )


def execute_orchestration(plan: OrchestrationPlan) -> Dict[str, Any]:
    """
    Execute the orchestration plan: Runway -> Stock -> Shotstack.
    
    NOTE: Runway API integration is stubbed pending API key configuration.
    When RUNWAY_API_KEY is set, this will make actual API calls.
    
    Pipeline:
    1. Generate visuals via Runway API (stubbed if no API key)
    2. Fetch stock assets based on orchestration queries
    3. Assemble via Shotstack API
    
    Args:
        plan: The complete orchestration plan
        
    Returns:
        Execution result with final video URL or render status
    """
    results: Dict[str, Any] = {
        "status": "processing",
        "runway_outputs": [],
        "stock_assets": [],
        "shotstack_result": None,
        "api_status": {
            "runway": "stubbed" if not RUNWAY_API_KEY else "ready",
            "shotstack": "stubbed" if not SHOTSTACK_API_KEY else "ready"
        }
    }
    
    if RUNWAY_API_KEY:
        print("[Execute] Runway API configured - would generate videos here")
    else:
        print("[Execute] Runway API not configured (RUNWAY_API_KEY missing) - returning stub data")
    
    for instr in plan.runway_instructions:
        results["runway_outputs"].append({
            "scene_id": instr.scene_id,
            "status": "pending_api_key" if not RUNWAY_API_KEY else "queued",
            "prompt": instr.prompt[:100],
            "duration": instr.duration,
            "generation_type": instr.generation_type
        })
    
    print(f"[Execute] Prepared {len(results['runway_outputs'])} Runway generation instructions")
    
    print("[Execute] Stock fetching based on orchestration queries...")
    for query in plan.stock_queries:
        results["stock_assets"].append({
            "query": query if isinstance(query, str) else str(query),
            "status": "pending"
        })
    
    if SHOTSTACK_API_KEY:
        print("[Execute] Shotstack API configured - ready for assembly")
    else:
        print("[Execute] Shotstack API not configured (SHOTSTACK_API_KEY missing)")
    
    return results
